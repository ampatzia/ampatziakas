}
countries_no_activity<-function(x){
x<-tidyr::spread(x,hs96,export_val,fill=0)
country_names<-x$origin
merc_names<-colnames(x)
x<-select(x,-origin)
x<-apply(x,2,as.logical)
x<-apply(x,2,as.numeric)
x<-as.data.frame(x)
a<- sum(rowSums(x)==0)
return(a)
}
nexp_list<-lapply(exp_list, make_nets)
x<-exp_list[[1]]
x<-tidyr::spread(x,hs96,export_val,fill=0)
country_names<-x$origin
merc_names<-colnames(x)
x<-select(x,-origin)
x<-apply(x,2,as.logical)
x<-apply(x,2,as.numeric)
x<-as.data.frame(x)
View(x)
row.names(x)<-country_names
x<-x[!rowSums(x)==0,]
g<-graph.incidence(x)
make_nets<- function(x){
x<-tidyr::spread(x,hs96,export_val,fill=0)
country_names<-x$origin
merc_names<-colnames(x)
x<-select(x,-origin)
x<-apply(x,2,as.logical)
x<-apply(x,2,as.numeric)
x<-as.data.frame(x)
row.names(x)<-country_names
x<-x[!rowSums(x)==0,]
g<-graph.incidence(x)
return(g)
}
nexp_list<-lapply(exp_list, make_nets)
g1<-nexp_list[[1]]
a<-cluster_leading_eigen(g1)
a
membership(a)
modularity(a)
aa<-modularity(a)
?modularity
summary(g1)
eigen_list<-lapply(nexp_list,cluster_leading_eigen)
greedy_list <- lapply(nexp_list,cluster_fast_greedy)
lp_list <- lapply(nexp_list,label.propagation.community)
algorithm(a)
edges_no<-lapply(nexp_list,g.ecount)
?g.count
edges_no<-lapply(nexp_list,gorder)
edges_no<-lapply(nexp_list,gsize)
View(x)
vertices_no<-c(lapply(nexp_list,gorder))
edges_no<-lapply(nexp_list,gsize)
null_mod<- function(x,y,num){
# x is vertices no
# y is edges no
a<-list()
vert1<- x[[num]]-15
vert2<- 15
edge_no <- y[[num]]
for(i in 1:1000){
a[[i]]<-sample_bipartite(vert1, vert2, type = c( "gnm"), edge_no, directed = FALSE)
}
}
null_mod<- function(x,y,num){
# x is vertices no
# y is edges no
a<-list()
vert1<- x[[num]]-15
vert2<- 15
edge_no <- y[[num]]
for(i in 1:1000){
a[[i]]<-sample_bipartite(vert1, vert2, type = c( "gnm"), edge_no, directed = FALSE)
}
return(a)
}
null_mod<- function(x,y,num){
# x is vertices no
# y is edges no
a<-list()
vert1<- x[[num]]-15
vert2<- 15
edge_no <- y[[num]]
for(i in 1:1000){
a[[i]]<-sample_bipartite(vert1, vert2, type = c( "gnm"), edge_no, directed = FALSE)
}
return(a)
}
list_2000<- null_mod(vertices_no,edges_no,1)
?sample_bipartite
null_mod<- function(x,y,num){
# x is vertices no
# y is edges no
a<-list()
vert1<- x[[num]]-15
vert2<- 15
edge_no <- y[[num]]
for(i in 1:1000){
a[[i]]<-sample_bipartite(vert1, vert2, m= edge_no, directed = FALSE)
}
return(a)
}
list_2000<- null_mod(vertices_no,edges_no,1)
null_mod<- function(x,y,num){
# x is vertices no
# y is edges no
a<-list()
vert1<- x[[num]]-15
vert2<- 15
edge_no <- y[[num]]
for(i in 1:1000){
a[[i]]<-sample_bipartite(vert1, vert2,type = c("gnm"), m= edge_no, directed = FALSE)
}
return(a)
}
list_2000<- null_mod(vertices_no,edges_no,1)
library(ggplot2)
library(knitr)
library(rmarkdown)
load("C:/Users/Kostas/Desktop/Bipartite_Trade/exp_eig_mod.Rda")
load("C:/Users/Kostas/Desktop/4_test.Rda")
micropan::binomixEstimate()
micropan::binomixEstimate
micropan:::binomixMachine
?invisible
pm_fluidity_all(bac_stre)
library(pasaR)
library(knitr)
library(dendextend)
knitr::opts_chunk$set(dev = `png`)
data( "bac_stre" )
data( "bac_stre_names" )
pm_fluidity_all(bac_stre)
a<-pm_fluidity_all(bac_stre)
install.packages("installr")
library("installr", lib.loc="~/R/win-library/3.4")
check.for.updates.R
check.for.updates.R()
install.MikTeX()
updateR()
?norm\
?norm
?rnorm
rnorm(seq(300,900,0.5),mean=535.42,sd=96.19)
a1<-rnorm(seq(300,900,0.5),mean=535.42,sd=96.19)
a2<-rnorm(seq(300,900,0.5),mean=775.24,sd= 218.37)
plot(a1)
plot(seq(300,900,0.5),a1,type = "n")
plot(seq(300,900,0.5),a1,type = "n")
lines(seq(300,900,0.5), a1)
lines(seq(300,900,0.5), a2)
a1<-dnorm(seq(300,900,0.5),mean=535.42,sd=96.19)
a2<-dnorm(seq(300,900,0.5),mean=775.24,sd= 218.37)
plot(seq(300,900,0.5),a1,type = "n")
lines(seq(300,900,0.5), a1)
lines(seq(300,900,0.5), a2)
x1<-seq(300,1800,0.5)
a1<-dnorm(x1,mean=535.42,sd=96.19)
a2<-dnorm(x1,mean=775.24,sd= 218.37)
plot(x1,a1,type = "n")
lines(x1, a1)
lines(x1, a2)
96.19
x1<-seq(300,1500,0.5)
a1<-dnorm(x1,mean=535.42,sd=96.19)
a2<-dnorm(x1,mean=775.24,sd= 218.37)
plot(x1,a1,type = "n")
lines(x1, a1)
lines(x1, a2)
library(dplyr)
library(readxl)
library(readxl)
a1_BHN_Quality <- read_excel("C:/Users/Kostas/Desktop/Tomato/data/a1_BHN_Quality.xlsx",
col_types = c("text", "text", "text",
"numeric", "text", "numeric"), na = "NA")
View(a1_BHN_Quality)
df1 <- read_excel("C:/Users/Kostas/Desktop/Tomato/data/a1_BHN_Quality.xlsx",
col_types = c("text", "text", "text",
"numeric", "text", "numeric"), na = "NA")
df1 %>% group_by(., Type,Day, `Temp_(C)` )%>%summarise(summarise(n=n() ,Mean=mean(Quality), Standard_Deviation=sd(Quality),Standard_Error=sd(Quality)/n())
df1 %>% group_by(., Type,Day, `Temp_(C)` )%>%
summarise(n=n() ,Mean=mean(Quality), Standard_Deviation=sd(Quality),Standard_Error=sd(Quality)/n())
bhn_q<-df1 %>% group_by(., Type,Day, `Temp_(C)` )%>%
summarise(n=n() ,Mean=mean(Quality), Standard_Deviation=sd(Quality),Standard_Error=sd(Quality)/sqrt(n()))
View(bhn_q)
df1<-df1[complete.cases(df1$Quality),]
bhn_q<-df1 %>% group_by(., Type,Day, `Temp_(C)` )%>%
summarise(n=n() ,Mean=mean(Quality), Standard_Deviation=sd(Quality),Standard_Error=sd(Quality)/sqrt(n()))
library(readxl)
a2_CHE_Quality <- read_excel("C:/Users/Kostas/Desktop/Tomato/data/a2_CHE_Quality.xlsx",
na = "NA")
View(a2_CHE_Quality)
library(readxl)
df3 <- read_excel("C:/Users/Kostas/Desktop/Tomato/data/a3_resp_che.xlsx")
View(df3)
df3 <- read_excel("C:/Users/Kostas/Desktop/Tomato/data/a3_resp_che.xlsx")
df3<-df3[complete.cases(df3$Respiration_Rate),]
che_q<-df3 %>% group_by(., Type,Day, `Temp_(C)` )%>%
summarise(n=n() ,Mean=mean(Respiration_Rate), Standard_Deviation=sd(Respiration_Rate),Standard_Error=sd(Respiration_Rate)/sqrt(n()))
df3 <- read_excel("C:/Users/Kostas/Desktop/Tomato/data/a3_resp_che.xlsx")
df3<-df3[complete.cases(df3$Respiration_Rate),]
che_resp<-df3 %>% group_by(., Type,Day, Degrees )%>%
summarise(n=n() ,Mean=mean(Respiration_Rate), Standard_Deviation=sd(Respiration_Rate),Standard_Error=sd(Respiration_Rate)/sqrt(n()))
che_resp
df4 <- read_excel("C:/Users/Kostas/Desktop/Tomato/data/a4_resp_bhn.xlsx")
df4<-df4[complete.cases(df4$Respiration_Rate),]
che_resp<-df4 %>% group_by(., Type,Day, Degrees )%>%
summarise(n=n() ,Mean=mean(Respiration_Rate), Standard_Deviation=sd(Respiration_Rate),Standard_Error=sd(Respiration_Rate)/sqrt(n()))
che_resp
library(readxl)
X03_texture_tomato_AFRI2016 <- read_excel("C:/Users/Kostas/Desktop/Tomato/data/03. texture_tomato AFRI2016.xlsx",
sheet = "BHN589 ")
View(X03_texture_tomato_AFRI2016)
df6<- read_excel("C:/Users/Kostas/Desktop/Tomato/data/03. texture_tomato AFRI2016.xlsx",
+     sheet = "Cherokee")
sheet = "BHN589 ")
df6<- read_excel("C:/Users/Kostas/Desktop/Tomato/data/03. texture_tomato AFRI2016.xlsx",
sheet = "Cherokee")
df5<- read_excel("C:/Users/Kostas/Desktop/Tomato/data/03. texture_tomato AFRI2016.xlsx",
sheet = "BHN589 ")
group_by(., Type,Day, Degrees )%>%
summarise(n=n() ,Mean=mean(`Force (N)`), Standard_Deviation=sd(`Force (N)`),Standard_Error=sd(`Force (N)`)/sqrt(n()))
df5 %>% group_by(., Type,Day, Degrees )%>%
summarise(n=n() ,Mean=mean(`Force (N)`), Standard_Deviation=sd(`Force (N)`),Standard_Error=sd(`Force (N)`)/sqrt(n()))
warnings()
df5<-df5[complete.cases(df5$`Force (N)` ),]
df5 %>% group_by(., Type,Day, Degrees )%>%
summarise(n=n() ,Mean=mean(`Force (N)`), Standard_Deviation=sd(`Force (N)`),Standard_Error=sd(`Force (N)`)/sqrt(n()))
warnings()
View(df5)
df5<- read_excel("C:/Users/Kostas/Desktop/Tomato/data/03. texture_tomato AFRI2016.xlsx",
sheet = "BHN589 ", na = "NA")
df5<-df5[complete.cases(df5$`Force (N)` ),]
che_force<-df5 %>% group_by(., Type,Day, Degrees )%>%
summarise(n=n() ,Mean=mean(`Force (N)`), Standard_Deviation=sd(`Force (N)`),Standard_Error=sd(`Force (N)`)/sqrt(n()))
df5 %>% group_by(., Type,Day, Degrees )%>%
summarise(n=n() ,Mean=mean(`Force (N)`), Standard_Deviation=sd(`Force (N)`),Standard_Error=sd(`Force (N)`)/sqrt(n()))
df7<-  read_excel("data/04. color_tomato AFRI2016.xlsx",
col_types = c("text", "text", "numeric",
"numeric", "text", "text", "numeric",
"numeric"))
df7<-read_excel("C:/Users/Kostas/Desktop/Tomato/data/04. color_tomato AFRI2016.xlsx",      sheet = "BHN589 - a", na = "NA")
df8<-read_excel("C:/Users/Kostas/Desktop/Tomato/data/04. color_tomato AFRI2016.xlsx",
sheet = "Cherokee purple - a", na = "NA")
library(readxl)
Questionaires <- read_excel("C:/Users/Kostas/Desktop/Xrysa/Questionaires.xlsx")
View(Questionaires)
summary(Questionaires)
?density
pdf_of_data <- density(Questionaires$Q1, from= min(Questionaires$Q1), to=max(Questionaires$Q1)+2 )
random.points <- approx(
cumsum(pdf_of_data$y)/sum(pdf_of_data$y),
pdf_of_data$x,
runif(10000)
)$y
library(readxl)
Questionaires <- read_excel("C:/Users/Kostas/Desktop/Xrysa/Questionaires.xlsx")
pdf_of_data <- density(Questionaires$Q1, from= min(Questionaires$Q1), to=max(Questionaires$Q1)+2 )
random.points <- approx(
cumsum(pdf_of_data$y)/sum(pdf_of_data$y),
pdf_of_data$x,
runif(13)
)$y
a1<-round(random.points,1)
plot(x=seq(0,13,1),x=seq(0,10,1))
?plot
plot(x=seq(1,13,1),y=Questionaires$Q1)
a1<-round(random.points,0)
lines(a1)
synth<-function(quest,add.max){
pdf_of_data <- density(quest, from= min(quest), to=max(quest)+add.max )
random.points <- approx(
cumsum(pdf_of_data$y)/sum(pdf_of_data$y),
pdf_of_data$x,
runif(13)
)$y
a1<-round(random.points,0)
return(a1)}
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
library(readxl)
Questionaires <- read_excel("C:/Users/Kostas/Desktop/Xrysa/Questionaires.xlsx")
synth<-function(quest,add.max){
pdf_of_data <- density(quest, from= 1, to=max(quest)+add.max )
random.points <- approx(
cumsum(pdf_of_data$y)/sum(pdf_of_data$y),
pdf_of_data$x,
runif(13)
)$y
a1<-round(random.points,0)
return(a1)}
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
synth(Questionaires$Q1,3)
sapply(Questionaires[,2:17],synth)
sapply(Questionaires[,2:17],function(x) synth(x,3))
set1<-sapply(Questionaires[,2:17],function(x) synth(x,3))
set2<-sapply(Questionaires[,2:17],function(x) synth(x,3))
set1<-as.data.frame(sapply(Questionaires[,2:17],function(x) synth(x,3)))
set2<-as.data.frame(sapply(Questionaires[,2:17],function(x) synth(x,3)))
View(set1)
library(dplyr)
bind_cols(Subject=seq(1,13,1),set1)
library(readxl)
library(dplyr)
Questionaires <- read_excel("C:/Users/Kostas/Desktop/Xrysa/Questionaires.xlsx")
synth<-function(quest,add.max){
pdf_of_data <- density(quest, from= 1, to=max(quest)+add.max )
random.points <- approx(
cumsum(pdf_of_data$y)/sum(pdf_of_data$y),
pdf_of_data$x,
runif(13)
)$y
a1<-round(random.points,0)
return(a1)}
synth(Questionaires$Q1,3)
set1<-as.data.frame(sapply(Questionaires[,2:17],function(x) synth(x,3)))
set2<-as.data.frame(sapply(Questionaires[,2:17],function(x) synth(x,3)))
set1<-bind_cols(Subject=seq(1,13,1),set1)
set2<-bind_cols(Subject=seq(1,13,1),set2)
library("xlsx", lib.loc="~/R/win-library/3.4")
write.xlsx(set1,file= "Questionaire_2.xlsx")
write.xlsx(set2,file= "Questionaire_3.xlsx")
?cc
library(readxl)
library(knitr)
?wilcox.test
#1. Read Data
df<- read_excel("C:/Users/Kostas/Desktop/Maria_Mourounoglou/maria_data.xls")
library(readxl)
library(knitr)
df<- read_excel("C:/Users/Kostas/Desktop/Maria_Mourounoglou/maria_data.xls")
a1<-wilcox.test(df$Histop.score[df$Group=="Habib-4X"],df$Histop.score[df$Group=="Control"])
a2<-wilcox.test(df$Histop.score[df$Group=="Habib-4X"],df$Histop.score[df$Group=="Crush-Clamp"])
a3<-wilcox.test(df$Histop.score[df$Group=="Crush-Clamp"],df$Histop.score[df$Group=="Control"])
df_Histop.score<-data.frame("Habib-4X"=c(NA,NA,NA),"Crush-Clamp"=c(NA,NA,NA),Control=c(NA,NA,NA))
df_Histop.score$Habib.4X<-c("-",round(a2$p.value , 5),round(a1$p.value , 5))
df_Histop.score$Crush.Clamp<-c(round(a2$p.value , 5),"-",round(a3$p.value , 5))
df_Histop.score$Control<-c(round(a1$p.value , 5),round(a3$p.value , 5),"-")
a1
a2
df_Histop.score$Habib.4X[df_Histop.score$Habib.4X> 0.05]<-"N.S."
df_Histop.score$Crush.Clamp[df_Histop.score$Crush.Clamp> 0.05] <-"N.S."
df_Histop.score$Control[df_Histop.score$Control> 0.05] <- "N.S."
rownames(df_Histop.score)<-colnames(df_Histop.score)
kable(df_NFkB,digits = 5 ,align="c",caption="Τιμές p στατιστικών συγκρίσεων ως προς την έκφραση
Histop.Score (Ν.S. : non-significant)")
kable(df_Histop,digits = 5 ,align="c",caption="Τιμές p στατιστικών συγκρίσεων ως προς την έκφραση Histop.Score (Ν.S. : non-significant)")
kable(df_Histop.score,digits = 5 ,align="c",caption="Τιμές p στατιστικών συγκρίσεων ως προς την έκφραση Histop.Score (Ν.S. : non-significant)")
library(rpart)
library(caret)
library(rpart)
library(caret)
library(rpart.plot)
library(CORElearn)
library(MASS)
library(mlbench)
library(alr3)
library(dplyr)
library(purrr)
set.seed(123)
data(ptitanic)
split_indices<-createDataPartition(ptitanic$survived, p = .7, list = FALSE,times = 1)
train_set <- ptitanic[split_indices, ]
test_set <- ptitanic[-split_indices, ]
#Useless?
#train<-train[complete.cases(train),]
# rownames(train_set)<-c(1:nrow(train))
# rownames(test_set)<-c(1:nrow(test))
grid_search_vars <- list(minNodeWeightRF = seq(1,7,1),
rfNoTrees = seq(10,200,25),estimators= c("Gini","ReliefFbestK")) %>%  purrr::cross_df()
fit_mod<-function(x,y,z){
fit.rand.forest = CoreModel(survived~., data=train_set, model="rf", selectionEstimator=z, minNodeWeightRF=x, rfNoTrees=y)
return(fit.rand.forest)
}
tm<-proc.time()
res<-pmap(list(grid_search_vars$minNodeWeightRF,grid_search_vars$rfNoTrees,grid_search_vars$estimators),fit_mod)
proc.time()-tm
a<- lapply(res, function(x) predict(x, newdata=test_set, type="class") )
b<-lapply(a, function(x) as.data.frame(t(c(confusionMatrix(x,test_set$survived)$overall,
confusionMatrix(x,test_set$survived)$byClass))))%>%bind_rows()
df<-bind_cols(grid_search_vars,b)%>% arrange(desc(Accuracy), rfNoTrees, minNodeWeightRF)
df
View(df)
ggplot(df,aes(x=rfNoTrees,y=Accuracy+colour=estimators))+geom_line()+geom_point()+ facet_grid(minNodeWeightRF ~ .)
ggplot(df,aes(x=rfNoTrees,y=Accuracy,colour=estimators))+geom_line()+geom_point()+ facet_grid(minNodeWeightRF ~ .)
ggplot(df,aes(x=rfNoTrees,y=Accuracy,colour=estimators))+
geom_line()+geom_point()+ geom_hline(aes(yintercept=max(df$Accuracy)), colour="#990000", linetype="dashed")+
facet_grid(minNodeWeightRF ~ .)
library(ggthemes)
library(rpart)
library(caret)
library(rpart.plot)
library(CORElearn)
library(MASS)
library(mlbench)
library(alr3)
library(dplyr)
library(purrr)
library(ggthemes)
set.seed(123)
data(ptitanic)
split_indices<-createDataPartition(ptitanic$survived, p = .7, list = FALSE,times = 1)
train_set <- ptitanic[split_indices, ]
test_set <- ptitanic[-split_indices, ]
#Useless?
#train<-train[complete.cases(train),]
# rownames(train_set)<-c(1:nrow(train))
# rownames(test_set)<-c(1:nrow(test))
grid_search_vars <- list(minNodeWeightRF = seq(1,7,1),
rfNoTrees = seq(10,200,25),estimators= c("Gini","ReliefFbestK", "MDL")) %>%  purrr::cross_df()
fit_mod<-function(x,y,z){
fit.rand.forest = CoreModel(survived~., data=train_set, model="rf", selectionEstimator=z, minNodeWeightRF=x, rfNoTrees=y)
return(fit.rand.forest)
}
tm<-proc.time()
res<-pmap(list(grid_search_vars$minNodeWeightRF,grid_search_vars$rfNoTrees,grid_search_vars$estimators),fit_mod)
proc.time()-tm
a<- lapply(res, function(x) predict(x, newdata=test_set, type="class") )
b<-lapply(a, function(x) as.data.frame(t(c(confusionMatrix(x,test_set$survived)$overall,
confusionMatrix(x,test_set$survived)$byClass))))%>%bind_rows()
df<-bind_cols(grid_search_vars,b)%>% arrange(desc(Accuracy), rfNoTrees, minNodeWeightRF)
ggplot(df,aes(x=rfNoTrees,y=Accuracy,colour=estimators))+
geom_line()+geom_point()+ geom_hline(aes(yintercept=max(df$Accuracy)), colour="#990000", linetype="dashed")+
facet_grid(minNodeWeightRF ~ .)+ scale_colour_tableau('tableau10medium')
preds<- lapply(res, function(x) predict(x, newdata=test_set, type="class") ) %>%
lapply(a, function(x) as.data.frame(t(c(confusionMatrix(x,test_set$survived)$overall,
confusionMatrix(x,test_set$survived)$byClass))))%>%bind_rows()
df<-bind_cols(grid_search_vars,preds)%>% arrange(desc(Accuracy), rfNoTrees, minNodeWeightRF)
preds<- lapply(res, function(x) predict(x, newdata=test_set, type="class") ) %>%
lapply(., function(x) as.data.frame(t(c(confusionMatrix(x,test_set$survived)$overall,
confusionMatrix(x,test_set$survived)$byClass))))%>%bind_rows()
df<-bind_cols(grid_search_vars,preds)%>% arrange(desc(Accuracy), rfNoTrees, minNodeWeightRF)
library("blogdown", lib.loc="~/R/win-library/3.4")
setwd("C:/Users/Kostas/Desktop/Blog")
new_site(dir = ".", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "yihui/hugo-lithium-theme", theme_example = TRUE,
serve = interactive())
new_site(dir = ""C:/Users/Kostas/desktop/Rblog", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "yihui/hugo-lithium-theme", theme_example = TRUE,
serve = interactive())
new_site(dir = C:/Users/Kostas/desktop/Rblog", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "yihui/hugo-lithium-theme", theme_example = TRUE,
serve = interactive())
new_site(dir = "C:/Users/Kostas/desktop/Rblog", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "yihui/hugo-lithium-theme", theme_example = TRUE,
serve = interactive())
new_site(dir = "C:/Users/Kostas/desktop/Rblog", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "gcushen/hugo-academic", theme_example = TRUE,
serve = interactive())
new_site(dir = "C:/Users/Kostas/desktop/Rblog/", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "gcushen/hugo-academic", theme_example = TRUE,
serve = interactive())
new_site(dir = "C:/Users/Kostas/desktop/Rblog/", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "gcushen/hugo-academic", theme_example = TRUE,
serve = interactive())
new_site(dir = "C:/Users/Kostas/desktop/Rblog/", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "gcushen/hugo-academic", theme_example = TRUE,
serve = interactive())
new_site(dir = "C:/Users/Kostas/desktop/Rblog", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "gcushen/hugo-academic", theme_example = TRUE,
serve = interactive())
new_site(dir = "C:/Users/Kostas/desktop/Rblog", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "yihui/hugo-lithium-theme", theme_example = TRUE,
serve = interactive())
setwd("C:/Users/Kostas/Desktop/Rblog")
install_theme("gcushen/hugo-academic
")
?install_theme
install_theme(gcushen/hugo-academic)
install_theme("gcushen/hugo-academic")
library("blogdown", lib.loc="~/R/win-library/3.4")
new_post("tf-grid-search")
?new_post
new_post("tf-grid-search",ext='.Rmd')
build_site()
install_theme("gcushen/hugo-academic")
install_theme("gcushen/hugo-academic",force=TRUE)
build_site()
new_site(dir = "C:/Users/Kostas/desktop/Rblog", install_hugo = TRUE, format = "toml",
sample = TRUE, theme = "gcushen/hugo-academic", theme_example = TRUE,
serve = interactive())
blogdown::new_site(dir = "C:/Users/Kostas/desktop/Rblog", theme = "gcushen/hugo-academic")
blogdown::new_site(dir = "C:/Users/Kostas/desktop/Rblog", theme = "gcushen/hugo-academic")
build_site()
serve_site()
build_site()
serve_site()
devtools::install_github('rstudio/blogdown')
detach("package:blogdown", unload=TRUE)
library("blogdown", lib.loc="~/R/win-library/3.4")
install.packages(c("backports", "checkmate", "crayon", "curl", "ddalpha", "doParallel", "dplyr", "forecast", "ggrepel", "lava", "lme4", "quantmod", "Rcpp"))
library("blogdown", lib.loc="~/R/win-library/3.4")
build_site()
serve_site()
